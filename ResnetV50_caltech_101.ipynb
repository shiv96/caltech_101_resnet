{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "ResnetV50_caltech_101.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRl90NQC1lOT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "ef806d66-d57c-4729-a952-e3c0320b33f8"
      },
      "source": [
        "# For downloading dataset\n",
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "\n",
        "# For extracting dataset\n",
        "import tarfile\n",
        "\n",
        "# For reading images\n",
        "import cv2\n",
        "\n",
        "# Essentials :)\n",
        "import numpy as np\n",
        "\n",
        "# pretty printing python objects\n",
        "import pprint\n",
        "\n",
        "# for sorting dictionary by value\n",
        "import operator\n",
        "\n",
        "# for showing images inline\n",
        "from matplotlib.pyplot import imshow \n",
        "%matplotlib inline \n",
        "\n",
        "# for making labels one-hot encoded\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# for splitting data into training and validation data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for CNN and NN models\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, Input, Dropout, Activation, Dense, MaxPooling2D, Flatten, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model\n",
        "\n",
        "# For transfer learning\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# to save models\n",
        "import json\n",
        "\n",
        "# for saving environment of notebook\n",
        "import dill\n",
        "\n",
        "# for printing size each variable is using\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4-sRIjR1lOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "255dc638-e414-4914-8dae-c4a397428025"
      },
      "source": [
        "URL_CALTECH_101_DATA = 'http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz'\n",
        "\n",
        "def download_dataset(url):\n",
        "    current_directory = os.path.dirname(os.path.realpath('__file__'))\n",
        "    dataset_file_path = current_directory+\"/dataset.tgz\"\n",
        "    if os.path.exists(dataset_file_path):\n",
        "        print(\"Already downloaded.\")\n",
        "    else:\n",
        "        filename, headers = urlretrieve(url, dataset_file_path)        \n",
        "    print(\"Done\")\n",
        "\n",
        "download_dataset(URL_CALTECH_101_DATA)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCWA_O411lOd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e72e1e5f-b8ae-4edc-bcd8-6f33048eb4dc"
      },
      "source": [
        "def extract_dataset(dataset_file_path, extraction_directory):\n",
        "    if (not os.path.exists(extraction_directory)):\n",
        "        os.makedirs(extraction_directory)\n",
        "    if (dataset_file_path.endswith(\"tar.gz\") or dataset_file_path.endswith(\".tgz\")):\n",
        "        tar = tarfile.open(dataset_file_path, \"r:gz\")\n",
        "        tar.extractall(path=extraction_directory)\n",
        "        tar.close()\n",
        "    elif (dataset_file_path.endswith(\"tar\")):\n",
        "        tar = tarfile.open(dataset_file_path, \"r:\")\n",
        "        tar.extractall(path=extraction_directory)\n",
        "        tar.close()\n",
        "    print(\"Done\")\n",
        "\n",
        "extract_dataset('./dataset.tgz','./data/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTBQE0ZB1lOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_images(object_category, data_directory):\n",
        "    if (not os.path.exists(data_directory)):\n",
        "        print(\"Data directory not found. Are you sure you downloaded and extracted dataset properly?\")\n",
        "        return\n",
        "    obj_category_dir = os.path.join(os.path.join(data_directory,\"101_ObjectCategories\"),object_category)\n",
        "    images = [os.path.join(obj_category_dir,img) for img in os.listdir(obj_category_dir)]\n",
        "    return images\n",
        "\n",
        "def read_image(image_path):\n",
        "    \"\"\"Read and resize individual images - Caltech 101 avg size of image is 300x200, so we resize accordingly\"\"\"\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "    img = cv2.resize(img, (300,200), interpolation=cv2.INTER_CUBIC)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QunV0sRa1lO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_images_per_category(data_directory):\n",
        "    categories = os.listdir(data_directory+\"/101_ObjectCategories/\")\n",
        "    object_images_count_dict = {}\n",
        "    for category in categories:\n",
        "        object_images_count_dict[category] = len(os.listdir(data_directory+\"/101_ObjectCategories/\"+category))\n",
        "    object_images_count_dict = sorted(object_images_count_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    return object_images_count_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCkVd2-l1lO-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e7e59ab-bc83-4838-c42a-cf836c68e380"
      },
      "source": [
        "return_images_per_category('./data')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('airplanes', 800),\n",
              " ('Motorbikes', 798),\n",
              " ('BACKGROUND_Google', 468),\n",
              " ('Faces_easy', 435),\n",
              " ('Faces', 435),\n",
              " ('watch', 239),\n",
              " ('Leopards', 200),\n",
              " ('bonsai', 128),\n",
              " ('car_side', 123),\n",
              " ('ketch', 114),\n",
              " ('chandelier', 107),\n",
              " ('hawksbill', 100),\n",
              " ('grand_piano', 99),\n",
              " ('brain', 98),\n",
              " ('butterfly', 91),\n",
              " ('helicopter', 88),\n",
              " ('menorah', 87),\n",
              " ('trilobite', 86),\n",
              " ('starfish', 86),\n",
              " ('kangaroo', 86),\n",
              " ('buddha', 85),\n",
              " ('sunflower', 85),\n",
              " ('ewer', 85),\n",
              " ('scorpion', 84),\n",
              " ('revolver', 82),\n",
              " ('laptop', 81),\n",
              " ('ibis', 80),\n",
              " ('llama', 78),\n",
              " ('minaret', 76),\n",
              " ('umbrella', 75),\n",
              " ('electric_guitar', 75),\n",
              " ('crab', 73),\n",
              " ('crayfish', 70),\n",
              " ('cougar_face', 69),\n",
              " ('dragonfly', 68),\n",
              " ('ferry', 67),\n",
              " ('dalmatian', 67),\n",
              " ('flamingo', 67),\n",
              " ('lotus', 66),\n",
              " ('dolphin', 65),\n",
              " ('elephant', 64),\n",
              " ('stop_sign', 64),\n",
              " ('euphonium', 64),\n",
              " ('joshua_tree', 64),\n",
              " ('soccer_ball', 64),\n",
              " ('schooner', 63),\n",
              " ('chair', 62),\n",
              " ('lamp', 61),\n",
              " ('yin_yang', 60),\n",
              " ('cellphone', 59),\n",
              " ('wheelchair', 59),\n",
              " ('rhino', 59),\n",
              " ('stegosaurus', 59),\n",
              " ('sea_horse', 57),\n",
              " ('pyramid', 57),\n",
              " ('cup', 57),\n",
              " ('windsor_chair', 56),\n",
              " ('nautilus', 55),\n",
              " ('accordion', 55),\n",
              " ('bass', 54),\n",
              " ('hedgehog', 54),\n",
              " ('emu', 53),\n",
              " ('pizza', 53),\n",
              " ('dollar_bill', 52),\n",
              " ('crocodile_head', 51),\n",
              " ('gramophone', 51),\n",
              " ('camera', 50),\n",
              " ('crocodile', 50),\n",
              " ('rooster', 49),\n",
              " ('tick', 49),\n",
              " ('pagoda', 47),\n",
              " ('ceiling_fan', 47),\n",
              " ('cougar_body', 47),\n",
              " ('barrel', 47),\n",
              " ('beaver', 46),\n",
              " ('stapler', 45),\n",
              " ('flamingo_head', 45),\n",
              " ('pigeon', 45),\n",
              " ('mandolin', 43),\n",
              " ('brontosaurus', 43),\n",
              " ('cannon', 43),\n",
              " ('headphone', 42),\n",
              " ('ant', 42),\n",
              " ('anchor', 42),\n",
              " ('lobster', 41),\n",
              " ('mayfly', 40),\n",
              " ('saxophone', 40),\n",
              " ('scissors', 39),\n",
              " ('wrench', 39),\n",
              " ('okapi', 39),\n",
              " ('panda', 38),\n",
              " ('water_lilly', 37),\n",
              " ('strawberry', 35),\n",
              " ('snoopy', 35),\n",
              " ('octopus', 35),\n",
              " ('wild_cat', 34),\n",
              " ('gerenuk', 34),\n",
              " ('garfield', 34),\n",
              " ('platypus', 34),\n",
              " ('binocular', 33),\n",
              " ('metronome', 32),\n",
              " ('inline_skate', 31)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2dgogAX1lPJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d903bb9f-0205-42f4-babc-d8d0f6761a81"
      },
      "source": [
        "total_count = 0\n",
        "for category,count in return_images_per_category('./data'):\n",
        "    if category == 'BACKGROUND_Google':\n",
        "        continue;\n",
        "    total_count += count\n",
        "print(\"Total number of images in training data : \",total_count)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of images in training data :  8677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikHa3IEB1lPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_training_data(data_directory):\n",
        "    i = 0\n",
        "    X = np.ndarray((8677, 200, 300, 3), dtype=np.uint8)\n",
        "    Y = []\n",
        "    print(\"Preparing X and Y for dataset...\")\n",
        "    for category,_ in return_images_per_category(data_directory):\n",
        "        if category == 'BACKGROUND_Google':\n",
        "            continue\n",
        "        print(\"Processing images of \",category)\n",
        "        for image in get_images(category, data_directory):\n",
        "            if not image.endswith('.jpg'):\n",
        "                # to escape hidden ipynb checkpoints and other unnecessary files \n",
        "                continue\n",
        "            X[i] = read_image(image)\n",
        "            Y.insert(i,category) \n",
        "            i += 1\n",
        "        print(\"Images processed : \",i+1,\" of 8678\")\n",
        "    print(\"Datasets constructed\")\n",
        "    return X,Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWaSIPQ_1lPT",
        "colab_type": "code",
        "colab": {},
        "outputId": "037ccf96-d045-47d0-8eb9-5d406e6b1182"
      },
      "source": [
        "X, Y = create_training_data('./data')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing X and Y for dataset...\n",
            "Processing images of  airplanes\n",
            "Images processed :  801  of 8678\n",
            "Processing images of  Motorbikes\n",
            "Images processed :  1599  of 8678\n",
            "Processing images of  Faces\n",
            "Images processed :  2034  of 8678\n",
            "Processing images of  Faces_easy\n",
            "Images processed :  2469  of 8678\n",
            "Processing images of  watch\n",
            "Images processed :  2708  of 8678\n",
            "Processing images of  Leopards\n",
            "Images processed :  2908  of 8678\n",
            "Processing images of  bonsai\n",
            "Images processed :  3036  of 8678\n",
            "Processing images of  car_side\n",
            "Images processed :  3159  of 8678\n",
            "Processing images of  ketch\n",
            "Images processed :  3273  of 8678\n",
            "Processing images of  chandelier\n",
            "Images processed :  3380  of 8678\n",
            "Processing images of  hawksbill\n",
            "Images processed :  3480  of 8678\n",
            "Processing images of  grand_piano\n",
            "Images processed :  3579  of 8678\n",
            "Processing images of  brain\n",
            "Images processed :  3677  of 8678\n",
            "Processing images of  butterfly\n",
            "Images processed :  3768  of 8678\n",
            "Processing images of  helicopter\n",
            "Images processed :  3856  of 8678\n",
            "Processing images of  menorah\n",
            "Images processed :  3943  of 8678\n",
            "Processing images of  trilobite\n",
            "Images processed :  4029  of 8678\n",
            "Processing images of  kangaroo\n",
            "Images processed :  4115  of 8678\n",
            "Processing images of  starfish\n",
            "Images processed :  4201  of 8678\n",
            "Processing images of  buddha\n",
            "Images processed :  4286  of 8678\n",
            "Processing images of  ewer\n",
            "Images processed :  4371  of 8678\n",
            "Processing images of  sunflower\n",
            "Images processed :  4456  of 8678\n",
            "Processing images of  scorpion\n",
            "Images processed :  4540  of 8678\n",
            "Processing images of  revolver\n",
            "Images processed :  4622  of 8678\n",
            "Processing images of  laptop\n",
            "Images processed :  4703  of 8678\n",
            "Processing images of  ibis\n",
            "Images processed :  4783  of 8678\n",
            "Processing images of  llama\n",
            "Images processed :  4861  of 8678\n",
            "Processing images of  minaret\n",
            "Images processed :  4937  of 8678\n",
            "Processing images of  electric_guitar\n",
            "Images processed :  5012  of 8678\n",
            "Processing images of  umbrella\n",
            "Images processed :  5087  of 8678\n",
            "Processing images of  crab\n",
            "Images processed :  5160  of 8678\n",
            "Processing images of  crayfish\n",
            "Images processed :  5230  of 8678\n",
            "Processing images of  cougar_face\n",
            "Images processed :  5299  of 8678\n",
            "Processing images of  dragonfly\n",
            "Images processed :  5367  of 8678\n",
            "Processing images of  dalmatian\n",
            "Images processed :  5434  of 8678\n",
            "Processing images of  ferry\n",
            "Images processed :  5501  of 8678\n",
            "Processing images of  flamingo\n",
            "Images processed :  5568  of 8678\n",
            "Processing images of  lotus\n",
            "Images processed :  5634  of 8678\n",
            "Processing images of  dolphin\n",
            "Images processed :  5699  of 8678\n",
            "Processing images of  elephant\n",
            "Images processed :  5763  of 8678\n",
            "Processing images of  joshua_tree\n",
            "Images processed :  5827  of 8678\n",
            "Processing images of  soccer_ball\n",
            "Images processed :  5891  of 8678\n",
            "Processing images of  stop_sign\n",
            "Images processed :  5955  of 8678\n",
            "Processing images of  euphonium\n",
            "Images processed :  6019  of 8678\n",
            "Processing images of  schooner\n",
            "Images processed :  6082  of 8678\n",
            "Processing images of  chair\n",
            "Images processed :  6144  of 8678\n",
            "Processing images of  lamp\n",
            "Images processed :  6205  of 8678\n",
            "Processing images of  yin_yang\n",
            "Images processed :  6265  of 8678\n",
            "Processing images of  wheelchair\n",
            "Images processed :  6324  of 8678\n",
            "Processing images of  cellphone\n",
            "Images processed :  6383  of 8678\n",
            "Processing images of  stegosaurus\n",
            "Images processed :  6442  of 8678\n",
            "Processing images of  rhino\n",
            "Images processed :  6501  of 8678\n",
            "Processing images of  cup\n",
            "Images processed :  6558  of 8678\n",
            "Processing images of  sea_horse\n",
            "Images processed :  6615  of 8678\n",
            "Processing images of  pyramid\n",
            "Images processed :  6672  of 8678\n",
            "Processing images of  windsor_chair\n",
            "Images processed :  6728  of 8678\n",
            "Processing images of  nautilus\n",
            "Images processed :  6783  of 8678\n",
            "Processing images of  accordion\n",
            "Images processed :  6838  of 8678\n",
            "Processing images of  bass\n",
            "Images processed :  6892  of 8678\n",
            "Processing images of  hedgehog\n",
            "Images processed :  6946  of 8678\n",
            "Processing images of  pizza\n",
            "Images processed :  6999  of 8678\n",
            "Processing images of  emu\n",
            "Images processed :  7052  of 8678\n",
            "Processing images of  dollar_bill\n",
            "Images processed :  7104  of 8678\n",
            "Processing images of  gramophone\n",
            "Images processed :  7155  of 8678\n",
            "Processing images of  crocodile_head\n",
            "Images processed :  7206  of 8678\n",
            "Processing images of  camera\n",
            "Images processed :  7256  of 8678\n",
            "Processing images of  crocodile\n",
            "Images processed :  7306  of 8678\n",
            "Processing images of  tick\n",
            "Images processed :  7355  of 8678\n",
            "Processing images of  rooster\n",
            "Images processed :  7404  of 8678\n",
            "Processing images of  barrel\n",
            "Images processed :  7451  of 8678\n",
            "Processing images of  cougar_body\n",
            "Images processed :  7498  of 8678\n",
            "Processing images of  ceiling_fan\n",
            "Images processed :  7545  of 8678\n",
            "Processing images of  pagoda\n",
            "Images processed :  7592  of 8678\n",
            "Processing images of  beaver\n",
            "Images processed :  7638  of 8678\n",
            "Processing images of  stapler\n",
            "Images processed :  7683  of 8678\n",
            "Processing images of  pigeon\n",
            "Images processed :  7728  of 8678\n",
            "Processing images of  flamingo_head\n",
            "Images processed :  7773  of 8678\n",
            "Processing images of  brontosaurus\n",
            "Images processed :  7816  of 8678\n",
            "Processing images of  mandolin\n",
            "Images processed :  7859  of 8678\n",
            "Processing images of  cannon\n",
            "Images processed :  7902  of 8678\n",
            "Processing images of  headphone\n",
            "Images processed :  7944  of 8678\n",
            "Processing images of  anchor\n",
            "Images processed :  7986  of 8678\n",
            "Processing images of  ant\n",
            "Images processed :  8028  of 8678\n",
            "Processing images of  lobster\n",
            "Images processed :  8069  of 8678\n",
            "Processing images of  saxophone\n",
            "Images processed :  8109  of 8678\n",
            "Processing images of  mayfly\n",
            "Images processed :  8149  of 8678\n",
            "Processing images of  okapi\n",
            "Images processed :  8188  of 8678\n",
            "Processing images of  wrench\n",
            "Images processed :  8227  of 8678\n",
            "Processing images of  scissors\n",
            "Images processed :  8266  of 8678\n",
            "Processing images of  panda\n",
            "Images processed :  8304  of 8678\n",
            "Processing images of  water_lilly\n",
            "Images processed :  8341  of 8678\n",
            "Processing images of  strawberry\n",
            "Images processed :  8376  of 8678\n",
            "Processing images of  octopus\n",
            "Images processed :  8411  of 8678\n",
            "Processing images of  snoopy\n",
            "Images processed :  8446  of 8678\n",
            "Processing images of  wild_cat\n",
            "Images processed :  8480  of 8678\n",
            "Processing images of  gerenuk\n",
            "Images processed :  8514  of 8678\n",
            "Processing images of  platypus\n",
            "Images processed :  8548  of 8678\n",
            "Processing images of  garfield\n",
            "Images processed :  8582  of 8678\n",
            "Processing images of  binocular\n",
            "Images processed :  8615  of 8678\n",
            "Processing images of  metronome\n",
            "Images processed :  8647  of 8678\n",
            "Processing images of  inline_skate\n",
            "Images processed :  8678  of 8678\n",
            "Datasets constructed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wam5LALb1lPY",
        "colab_type": "code",
        "colab": {},
        "outputId": "d94b2b20-dcec-484a-b341-da8889472e20"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8677, 200, 300, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-SKWlBG1lP3",
        "colab_type": "code",
        "colab": {},
        "outputId": "3de59aa3-0377-480a-c1a3-fc8a79be5a91"
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "Y_integer_encoded = label_encoder.fit_transform(Y)\n",
        "Y_integer_encoded"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5,  5,  5, ..., 52, 52, 52])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJHYGUDE1lP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('label_encoder.npy', label_encoder.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quxw4KxN1lQP",
        "colab_type": "code",
        "colab": {},
        "outputId": "6280cb0a-33fb-43d5-80bd-affdc6c698ab"
      },
      "source": [
        "Y_one_hot = to_categorical(Y_integer_encoded)\n",
        "Y_one_hot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
              "       ..., \n",
              "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
              "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ndAXKD01lQU",
        "colab_type": "code",
        "colab": {},
        "outputId": "b321ad48-a9a1-40c8-9370-1823562c9643"
      },
      "source": [
        "Y_one_hot[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE6MNrmE1lQZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "3e619030-3619-414e-aaa3-0667cafee0bf"
      },
      "source": [
        "len(Y_one_hot[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3YAE0ME1lQg",
        "colab_type": "code",
        "colab": {},
        "outputId": "6e2ed00d-9493-4a8c-d76c-0cd3e16518a8"
      },
      "source": [
        "label_encoder.inverse_transform(np.argmax(Y_one_hot[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'airplanes'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSQffB9L1lQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_normalized = X.astype(np.float64) / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU6Q9y1y1lQq",
        "colab_type": "code",
        "colab": {},
        "outputId": "96ecd373-70a3-4825-9499-a4372a5dd4e4"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[126, 117, 114],\n",
              "        [129, 120, 117],\n",
              "        [130, 121, 118],\n",
              "        ..., \n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[126, 117, 114],\n",
              "        [129, 120, 117],\n",
              "        [130, 121, 118],\n",
              "        ..., \n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[127, 118, 115],\n",
              "        [130, 121, 118],\n",
              "        [131, 122, 119],\n",
              "        ..., \n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       ..., \n",
              "       [[ 81, 113, 130],\n",
              "        [ 80, 112, 129],\n",
              "        [ 80, 110, 128],\n",
              "        ..., \n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[ 84, 117, 133],\n",
              "        [ 79, 113, 128],\n",
              "        [ 79, 110, 126],\n",
              "        ..., \n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]],\n",
              "\n",
              "       [[ 87, 122, 136],\n",
              "        [ 79, 115, 128],\n",
              "        [ 78, 111, 126],\n",
              "        ..., \n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255],\n",
              "        [255, 255, 255]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzNandM71lQy",
        "colab_type": "code",
        "colab": {},
        "outputId": "5bbf905f-8056-4a31-85cf-bfe249411abb"
      },
      "source": [
        "X_normalized[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.49411765,  0.45882353,  0.44705882],\n",
              "        [ 0.50588235,  0.47058824,  0.45882353],\n",
              "        [ 0.50980392,  0.4745098 ,  0.4627451 ],\n",
              "        ..., \n",
              "        [ 1.        ,  1.        ,  1.        ],\n",
              "        [ 1.        ,  1.        ,  1.        ],\n",
              "        [ 1.        ,  1.        ,  1.        ]],\n",
              "\n",
              "       [[ 0.49411765,  0.45882353,  0.44705882],\n",
              "        [ 0.50588235,  0.47058824,  0.45882353],\n",
              "        [ 0.50980392,  0.4745098 ,  0.4627451 ],\n",
              "        ..., \n",
              "        [ 1.        ,  1.        ,  1.        ],\n",
              "        [ 1.        ,  1.        ,  1.        ],\n",
              "        [ 1.        ,  1.        ,  1.        ]],\n",
              "\n",
              "       [[ 0.49803922,  0.4627451 ,  0.45098039],\n",
              "        [ 0.50980392,  0.4745098 ,  0.4627451 ],\n",
              "        [ 0.51372549,  0.47843137,  0.46666667],\n",
              "        ..., \n",
              "        [ 1.        ,  1.        ,  1.        ],\n",
              "        [ 1.        ,  1.        ,  1.        ],\n",
              "        [ 1.        ,  1.        ,  1.        ]],\n",
              "\n",
              "       ..., \n",
              "       [[ 0.31764706,  0.44313725,  0.50980392],\n",
              "        [ 0.31372549,  0.43921569,  0.50588235],\n",
              "        [ 0.31372549,  0.43137255,  0.50196078],\n",
              "        ..., \n",
              "        [ 1.        ,  1.        ,  1.        ],\n",
              "        [ 1.        ,  1.        ,  1.        ],\n",
              "        [ 1.        ,  1.        ,  1.        ]],\n",
              "\n",
              "       [[ 0.32941176,  0.45882353,  0.52156863],\n",
              "        [ 0.30980392,  0.44313725,  0.50196078],\n",
              "        [ 0.30980392,  0.43137255,  0.49411765],\n",
              "        ..., \n",
              "        [ 1.        ,  1.        ,  1.        ],\n",
              "        [ 1.        ,  1.        ,  1.        ],\n",
              "        [ 1.        ,  1.        ,  1.        ]],\n",
              "\n",
              "       [[ 0.34117647,  0.47843137,  0.53333333],\n",
              "        [ 0.30980392,  0.45098039,  0.50196078],\n",
              "        [ 0.30588235,  0.43529412,  0.49411765],\n",
              "        ..., \n",
              "        [ 1.        ,  1.        ,  1.        ],\n",
              "        [ 1.        ,  1.        ,  1.        ],\n",
              "        [ 1.        ,  1.        ,  1.        ]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI1IUYH81lQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We wouldn't require X anymore, so, let's free up some memory\n",
        "del X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygGl9V-H1lQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X_normalized, Y_one_hot, test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAVT-WpY1lRA",
        "colab_type": "code",
        "colab": {},
        "outputId": "15108f7d-90cb-4aee-84d0-a8e82bb20731"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6507, 200, 300, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5rMQgYO1lRG",
        "colab_type": "code",
        "colab": {},
        "outputId": "4af9130b-3260-49f5-d2c2-48c8bc9e0657"
      },
      "source": [
        "Y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6507, 101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuYmWnEe1lRR",
        "colab_type": "code",
        "colab": {},
        "outputId": "2dc540d4-2d36-4765-840f-c98b165a02f4"
      },
      "source": [
        "label_encoder.inverse_transform(np.argmax(Y_train[6001]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Motorbikes'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h316mhns1lRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dill.dump_session('notebook_env_until_nn_models.db')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X7uGSgk1lR4",
        "colab_type": "code",
        "colab": {},
        "outputId": "d7362ea9-354f-455c-f348-c12490438dc3"
      },
      "source": [
        "# Architecture of ResNet\n",
        "model_cnn_dropout = Sequential()\n",
        "model_cnn_dropout.add(Conv2D(32, (3,3), activation='relu', input_shape=(200,300,3)))\n",
        "model_cnn_dropout.add(Conv2D(32, (3,3), activation='relu'))\n",
        "model_cnn_dropout.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_cnn_dropout.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model_cnn_dropout.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model_cnn_dropout.add(MaxPooling2D(pool_size=2, strides=2))\n",
        "model_cnn_dropout.add(Flatten())\n",
        "model_cnn_dropout.add(Dense(512, activation='relu'))\n",
        "model_cnn_dropout.add(Dropout(0.5))\n",
        "model_cnn_dropout.add(Dense(101, activation='softmax'))\n",
        "model_cnn_dropout.summary()\n",
        "\n",
        "# loss and optimizer\n",
        "model_cnn_dropout.compile(loss='categorical_crossentropy',optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "# training\n",
        "callbacks = [ModelCheckpoint('cnn_model_dropout_two.h5', monitor='val_acc', save_best_only=True),\n",
        "            EarlyStopping(monitor='val_loss', patience=4, verbose=1, mode='auto')]\n",
        "model_cnn_dropout.fit(X_train, Y_train, batch_size=16, epochs=10, verbose=1, validation_data=(X_validation,Y_validation), callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 198, 298, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 196, 296, 32)      9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 98, 148, 32)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 96, 146, 64)       18496     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 94, 144, 64)       36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 47, 72, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 216576)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               110887424 \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 101)               51813     \n",
            "=================================================================\n",
            "Total params: 111,004,805\n",
            "Trainable params: 111,004,805\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 6507 samples, validate on 2170 samples\n",
            "Epoch 1/10\n",
            "6507/6507 [==============================] - 106s - loss: 4.0300 - acc: 0.1778 - val_loss: 3.7339 - val_acc: 0.2622\n",
            "Epoch 2/10\n",
            "6507/6507 [==============================] - 99s - loss: 3.4675 - acc: 0.2783 - val_loss: 3.0735 - val_acc: 0.3567\n",
            "Epoch 3/10\n",
            "6507/6507 [==============================] - 99s - loss: 3.0919 - acc: 0.3406 - val_loss: 2.7875 - val_acc: 0.4051\n",
            "Epoch 4/10\n",
            "6507/6507 [==============================] - 99s - loss: 2.6679 - acc: 0.4154 - val_loss: 2.5430 - val_acc: 0.4410\n",
            "Epoch 5/10\n",
            "6507/6507 [==============================] - 99s - loss: 2.1795 - acc: 0.4955 - val_loss: 2.2610 - val_acc: 0.5009\n",
            "Epoch 6/10\n",
            "6507/6507 [==============================] - 99s - loss: 1.5803 - acc: 0.6173 - val_loss: 2.1526 - val_acc: 0.5120\n",
            "Epoch 7/10\n",
            "6507/6507 [==============================] - 99s - loss: 0.9767 - acc: 0.7515 - val_loss: 2.2493 - val_acc: 0.5198\n",
            "Epoch 8/10\n",
            "6507/6507 [==============================] - 99s - loss: 0.5546 - acc: 0.8551 - val_loss: 2.2918 - val_acc: 0.5217\n",
            "Epoch 9/10\n",
            "6507/6507 [==============================] - 98s - loss: 0.3581 - acc: 0.8966 - val_loss: 2.4754 - val_acc: 0.5300\n",
            "Epoch 10/10\n",
            "6507/6507 [==============================] - 98s - loss: 0.2363 - acc: 0.9382 - val_loss: 2.4870 - val_acc: 0.5585\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd1a1156240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLQSs_tF1lR8",
        "colab_type": "code",
        "colab": {},
        "outputId": "c38af3b3-c5aa-40ae-f8a3-1a2c13855376"
      },
      "source": [
        "# Train some more as val accuracy seems to be improving even though it still looks like overfit\n",
        "model_cnn_dropout.fit(X_train, Y_train, batch_size=16, epochs=10, verbose=1, validation_data=(X_validation,Y_validation), callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6507 samples, validate on 2170 samples\n",
            "Epoch 1/10\n",
            "6507/6507 [==============================] - 96s - loss: 0.1744 - acc: 0.9511 - val_loss: 2.3959 - val_acc: 0.5539\n",
            "Epoch 2/10\n",
            "6507/6507 [==============================] - 96s - loss: 0.1227 - acc: 0.9671 - val_loss: 2.4932 - val_acc: 0.5442\n",
            "Epoch 3/10\n",
            "6507/6507 [==============================] - 96s - loss: 0.0949 - acc: 0.9733 - val_loss: 2.4786 - val_acc: 0.5585\n",
            "Epoch 4/10\n",
            "6507/6507 [==============================] - 98s - loss: 0.1013 - acc: 0.9736 - val_loss: 2.4444 - val_acc: 0.5627\n",
            "Epoch 5/10\n",
            "6507/6507 [==============================] - 96s - loss: 0.0672 - acc: 0.9811 - val_loss: 2.6152 - val_acc: 0.5618\n",
            "Epoch 6/10\n",
            "6507/6507 [==============================] - 96s - loss: 0.0722 - acc: 0.9813 - val_loss: 2.5996 - val_acc: 0.5401\n",
            "Epoch 00005: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd1a131a710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cqDmYio1lSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn_dropout.save('model_Resnet_dropout_with_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ME14Mo81lSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json_cnn_dropout = model_Resnet_dropout.to_json()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEvXlG3u1lSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"model_Resnet_dropout.json\", \"w\") as f:\n",
        "    json.dump(json.loads(json_cnn_dropout), f, indent=4)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}